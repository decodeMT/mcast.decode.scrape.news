{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install pgvector in pgsql\n",
    "https://github.com/pgvector/pgvector?tab=readme-ov-file#installation\n",
    "\n",
    "\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "ALTER TABLE localnews.articles ADD COLUMN vector_2d vector(2);\n",
    "ALTER TABLE localnews.articles ADD COLUMN vector vector(768);\n",
    "ALTER TABLE localnews.articles ADD COLUMN similar_matches jsonb;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503235/503235 [4:53:32<00:00, 28.57it/s]   \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"decodeMT\",\n",
    "    user=\"postgres\",\n",
    "    password=\"rufy100\",\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch articles\n",
    "cursor.execute('select entryid, article from localnews.articles order by parseddate')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Process each article and update its vector in the database\n",
    "for row in tqdm(rows):\n",
    "    entryid, article = row  # Access the correct columns\n",
    "    encoding = model.encode(article)  # Get the article text\n",
    "\n",
    "    # Convert numpy array to list and format it as a PostgreSQL array\n",
    "    encoding_list = encoding.tolist()\n",
    "\n",
    "    # Update the article's vector column\n",
    "    cursor.execute('update localnews.articles set vector = %s where entryid = %s', (encoding_list, entryid))\n",
    "\n",
    "    # Commit changes to the database\n",
    "    conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alang\\miniconda3\\envs\\news_two\\Lib\\site-packages\\umap\\umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      "503235it [02:36, 3207.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import ast\n",
    "import umap\n",
    "\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"decodeMT\",\n",
    "    user=\"postgres\",\n",
    "    password=\"rufy100\",\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch articles\n",
    "cursor.execute('select entryid, vector from localnews.articles;')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "vectors = [ast.literal_eval(row[1]) for row in rows]\n",
    "vectors = np.array(vectors, dtype=float)\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# reduced_vectors = pca.fit_transform(vectors)\n",
    "umap_reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "reduced_vectors = umap_reducer.fit_transform(vectors)\n",
    "\n",
    "# Process each article and update its vector in the database\n",
    "for row, vector in tqdm(zip(rows, reduced_vectors)):\n",
    "\n",
    "    cursor.execute('update localnews.articles set vector_2d = %s where entryid = %s', (vector.tolist(), row[0]))\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503233/503233 [39:44:44<00:00,  3.52it/s]    \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import ast\n",
    "import json\n",
    "\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"decodeMT\",\n",
    "    user=\"postgres\",\n",
    "    password=\"rufy100\",\n",
    ")\n",
    "\n",
    "conn_sim = psycopg2.connect(\n",
    "    dbname=\"decodeMT\",\n",
    "    user=\"postgres\",\n",
    "    password=\"rufy100\",\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "conn_sim.autocommit = True\n",
    "cursor_sim = conn_sim.cursor()\n",
    "\n",
    "# Fetch articles\n",
    "cursor.execute(\"select entryid from localnews.articles order by parseddate , entryid\");\n",
    "rows = cursor.fetchall()\n",
    "for row in tqdm(rows):\n",
    "    cursor_sim.execute(\"\"\"\n",
    "                        WITH date_range AS (\n",
    "                            SELECT parseddate\n",
    "                            FROM localnews.articles\n",
    "                            WHERE entryid = %s\n",
    "                        ),\n",
    "                        temp_table AS (\n",
    "                            SELECT entryid, vector_2d, vector <=> (SELECT vector FROM localnews.articles WHERE entryid = %s) AS similarity\n",
    "                            FROM localnews.articles a, date_range\n",
    "                            WHERE vector IS NOT NULL\n",
    "                            AND a.parseddate BETWEEN date_range.parseddate - INTERVAL '6 months'\n",
    "                            AND date_range.parseddate + INTERVAL '6 months'\n",
    "                        )\n",
    "                        SELECT entryid, vector_2d, (1-similarity) AS \"similar\"\n",
    "                        FROM temp_table\n",
    "                        WHERE (1-similarity) >= 0.7\n",
    "                        AND entryid != %s\n",
    "                        ORDER BY \"similar\" DESC;\n",
    "                        \"\"\", (row[0], row[0], row[0]))\n",
    "\n",
    "    result = cursor_sim.fetchall()\n",
    "\n",
    "    processed_result = [\n",
    "        {\n",
    "            \"to\": row[0],\n",
    "            \"x\": round(ast.literal_eval(row[1])[0], 2),\n",
    "            \"y\": round(ast.literal_eval(row[1])[1], 2),\n",
    "            \"val\": round(row[2],2)\n",
    "        }\n",
    "        for row in result\n",
    "    ]\n",
    "\n",
    "    cursor_sim.execute('update localnews.articles set similar_articles = %s where entryid = %s', (json.dumps(processed_result), row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a table copy only required content. The final query is how to retrieve data.\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS news\n",
    "(\n",
    "    entryid bigint NOT NULL,\n",
    "    source character varying(6),\n",
    "    title character varying(2048),\n",
    "    parseddate date,\n",
    "    link text,\n",
    "    vector_2d text,\n",
    "    similar_articles text,\n",
    "    CONSTRAINT art_pk PRIMARY KEY (entryid)\n",
    ")\n",
    "\n",
    "\n",
    "insert into news\n",
    "select entryId, source, title, cast(parseddate as date), link, cast(vector_2d as text)\n",
    "\t\t, cast(similar_articles as text)\n",
    "from localnews.articles\n",
    "\n",
    "\n",
    "WITH relevant_ids AS (\n",
    "    SELECT entryid \n",
    "    from news\n",
    "   WHERE parseddate BETWEEN '2020-01-14'::DATE - INTERVAL '2 days' \n",
    "                      AND '2020-01-14'::DATE + INTERVAL '2 days'\n",
    "),\n",
    "filtered_matches AS (\n",
    "    SELECT \n",
    "        a.entryid, \n",
    "        a.source, \n",
    "        a.title, \n",
    "        TO_CHAR(a.parseddate::timestamp, 'yyyy-mm-dd') AS date, \n",
    "        a.link,\n",
    "        split_part(trim(both '[]' from a.vector_2d), ',', 1)::float AS x,\n",
    "        split_part(trim(both '[]' from a.vector_2d), ',', 2)::float AS y,\n",
    "        jsonb_agg(\n",
    "            jsonb_build_object('to', elements.value ->> 'to', 'val', elements.value ->> 'val')\n",
    "        ) AS filtered_similar_matches\n",
    "    FROM \n",
    "        news a\n",
    "    JOIN \n",
    "        jsonb_array_elements(a.similar_articles::jsonb) AS elements ON (elements.value ->> 'to')::int IN (SELECT entryid FROM relevant_ids)\n",
    "    JOIN \n",
    "        relevant_ids r ON a.entryid = r.entryid\n",
    "    WHERE \n",
    "        (elements.value ->> 'to')::int <> a.entryid\n",
    "\t\n",
    "    GROUP BY \n",
    "        a.entryid, a.source, a.title, TO_CHAR(a.parseddate::timestamp, 'yyyy-mm-dd'), a.link, a.vector_2d\n",
    ")\n",
    "SELECT * FROM filtered_matches order by entryid;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_two",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
