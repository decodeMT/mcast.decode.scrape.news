{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install pgvector in pgsql\n",
    "https://github.com/pgvector/pgvector?tab=readme-ov-file#installation\n",
    "\n",
    "\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "ALTER TABLE localnews.articles ADD COLUMN vector_2d vector(2);\n",
    "ALTER TABLE localnews.articles ADD COLUMN vector vector(768);\n",
    "ALTER TABLE localnews.articles ADD COLUMN similar_matches jsonb;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503235/503235 [4:53:32<00:00, 28.57it/s]   \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"decodeMT\",\n",
    "    user=\"postgres\",\n",
    "    password=\"rufy100\",\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch articles\n",
    "cursor.execute('select entryid, article from localnews.articles order by parseddate')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Process each article and update its vector in the database\n",
    "for row in tqdm(rows):\n",
    "    entryid, article = row  # Access the correct columns\n",
    "    encoding = model.encode(article)  # Get the article text\n",
    "\n",
    "    # Convert numpy array to list and format it as a PostgreSQL array\n",
    "    encoding_list = encoding.tolist()\n",
    "\n",
    "    # Update the article's vector column\n",
    "    cursor.execute('update localnews.articles set vector = %s where entryid = %s', (encoding_list, entryid))\n",
    "\n",
    "    # Commit changes to the database\n",
    "    conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alang\\miniconda3\\envs\\news_two\\Lib\\site-packages\\umap\\umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      "503235it [02:36, 3207.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import ast\n",
    "import umap\n",
    "\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"decodeMT\",\n",
    "    user=\"postgres\",\n",
    "    password=\"rufy100\",\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch articles\n",
    "cursor.execute('select entryid, vector from localnews.articles;')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "vectors = [ast.literal_eval(row[1]) for row in rows]\n",
    "vectors = np.array(vectors, dtype=float)\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# reduced_vectors = pca.fit_transform(vectors)\n",
    "umap_reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "reduced_vectors = umap_reducer.fit_transform(vectors)\n",
    "\n",
    "# Process each article and update its vector in the database\n",
    "for row, vector in tqdm(zip(rows, reduced_vectors)):\n",
    "\n",
    "    cursor.execute('update localnews.articles set vector_2d = %s where entryid = %s', (vector.tolist(), row[0]))\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total records to process: 503235\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Database connection parameters\n",
    "DB_NAME = \"decodeMT\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"rufy100\"\n",
    "\n",
    "# Define number of parallel workers and chunk size\n",
    "num_workers = 4\n",
    "chunk_size = 10000\n",
    "\n",
    "def process_chunk(worker_id):\n",
    "    # Connect to the database inside the worker function\n",
    "    conn = psycopg2.connect(dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "    conn_sim = psycopg2.connect(dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "    cursor = conn.cursor()\n",
    "    cursor_sim = conn.cursor()\n",
    "\n",
    "    offset = worker_id * chunk_size\n",
    "    cursor.execute('SELECT entryid, vector FROM localnews.articles WHERE vector IS NOT NULL LIMIT %s OFFSET %s', (chunk_size, offset))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    logging.info(f'Worker {worker_id}: Processing {len(rows)} records.')\n",
    "\n",
    "    for row in tqdm(rows):\n",
    "        entryid = row[0]\n",
    "        vector = row[1]\n",
    "        \n",
    "        cursor_sim.execute(\"\"\"\n",
    "                    WITH temp_table as (\n",
    "                    SELECT entryid, vector_2d, vector <=> (SELECT vector FROM localnews.articles WHERE entryid = %s) AS similarity\n",
    "                    FROM localnews.articles\n",
    "\t                WHERE vector is not null\n",
    "                    )\n",
    "                    SELECT entryid, vector_2d, (1-similarity) as \"similar\"\n",
    "                    FROM temp_table\n",
    "                    WHERE (1-similarity)>=0.7\n",
    "                    AND entryid != %s\n",
    "                    ORDER BY \"similar\" DESC\n",
    "                    \"\"\", (row[0], row[0]))\n",
    "\n",
    "        result = cursor_sim.fetchall()\n",
    "\n",
    "        processed_result = [\n",
    "            {\n",
    "                \"to\": row[0],\n",
    "                \"xy\": ast.literal_eval(row[1]),  # Convert string to actual list\n",
    "                \"val\": round(row[2],2)\n",
    "            }\n",
    "            for row in result\n",
    "        ]\n",
    "\n",
    "        # Update the database\n",
    "        cursor_sim.execute('UPDATE localnews.articles SET processed_column = %s WHERE entryid = %s', (json.dumps(processed_result), entryid))\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    logging.info(f'Worker {worker_id}: Finished processing.')\n",
    "\n",
    "\n",
    "# Get the total number of records\n",
    "with psycopg2.connect(dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD) as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT COUNT(*) FROM localnews.articles WHERE vector IS NOT NULL')\n",
    "    total_records = cursor.fetchone()[0]\n",
    "    cursor.close()\n",
    "\n",
    "logging.info(f'Total records to process: {total_records}')\n",
    "\n",
    "# Use parallel workers\n",
    "with Pool(num_workers) as pool:\n",
    "    pool.map(process_chunk, range(num_workers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21682/21682 [2:37:36<00:00,  2.29it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21355/21355 [2:27:47<00:00,  2.41it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28987/28987 [4:32:34<00:00,  1.77it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26731/26731 [2:44:31<00:00,  2.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29506/29506 [3:02:52<00:00,  2.69it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25512/25512 [2:37:16<00:00,  2.70it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23005/23005 [2:17:44<00:00,  2.78it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27147/27147 [3:15:22<00:00,  2.32it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24856/24856 [2:37:06<00:00,  2.64it/s]  \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import ast\n",
    "import json\n",
    "\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"decodeMT\",\n",
    "    user=\"postgres\",\n",
    "    password=\"rufy100\",\n",
    ")\n",
    "\n",
    "conn_sim = psycopg2.connect(\n",
    "    dbname=\"decodeMT\",\n",
    "    user=\"postgres\",\n",
    "    password=\"rufy100\",\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "conn_sim.autocommit = True\n",
    "cursor_sim = conn_sim.cursor()\n",
    "\n",
    "# Fetch articles\n",
    "cursor.execute(\"select distinct date_part('year', parseddate) from localnews.articles where date_part('year', parseddate)\")\n",
    "years = cursor.fetchall()\n",
    "for year in years:\n",
    "    print(int(year[0]))\n",
    "    cursor.execute(\"SELECT entryid, vector FROM localnews.articles WHERE date_part('year', parseddate)=%s\", (int(year[0]),))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    for row in tqdm(rows):\n",
    "        cursor_sim.execute(\"\"\"\n",
    "                    WITH temp_table as (\n",
    "                    SELECT entryid, vector_2d, vector <=> (SELECT vector FROM localnews.articles WHERE entryid = %s) AS similarity\n",
    "                    FROM localnews.articles\n",
    "\t                WHERE vector is not null AND date_part('year', parseddate)=%s\n",
    "                    )\n",
    "                    SELECT entryid, vector_2d, (1-similarity) as \"similar\"\n",
    "                    FROM temp_table\n",
    "                    WHERE (1-similarity)>=0.7\n",
    "                    AND entryid != %s\n",
    "                    ORDER BY \"similar\" DESC\n",
    "                    \"\"\", (row[0], int(year[0]), row[0]))\n",
    "\n",
    "        result = cursor_sim.fetchall()\n",
    "\n",
    "        processed_result = [\n",
    "            {\n",
    "                \"to\": row[0],\n",
    "                \"xy\": ast.literal_eval(row[1]),  # Convert string to actual list\n",
    "                \"val\": round(row[2],2)\n",
    "            }\n",
    "            for row in result\n",
    "        ]\n",
    "\n",
    "        cursor_sim.execute('update localnews.articles set similar_articles = %s where entryid = %s', (json.dumps(processed_result), row[0]))\n",
    "    \n",
    "#2001\n",
    "#2002\n",
    "#2003\n",
    "#2004\n",
    "#2005\n",
    "#2008\n",
    "#2009\n",
    "#2011\n",
    "#2012\n",
    "#2014\n",
    "#2019\n",
    "#2020\n",
    "#2021\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
