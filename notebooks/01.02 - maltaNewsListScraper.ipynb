{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script has been created to extract the article date from the ToM articles due to an early bug in the article scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(fileName: str, response: str):\n",
    "    '''\n",
    "    Saves an HTTP request response to file.\n",
    "    Arguments:\n",
    "        fileName: the name with path where to save.\n",
    "        response: the HTTP request response to save.\n",
    "    '''\n",
    "    # Open file and write file\n",
    "    with open(fileName, \"w\", encoding=\"utf-8\") as pf:\n",
    "        pf.write(\"{}\\n\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeNewsList(page: int, source: str, url: str, errorFolder: str, timestamp: str) -> str:\n",
    "    '''\n",
    "    A basic scraper for news links.\n",
    "    Arguments:\n",
    "        page: the page number being scraped.\n",
    "        source: the name of the source.\n",
    "        url: the URL to be scraped. Source specific, with page to be injected.\n",
    "        errorFolder: the folder where errors are to be saved.\n",
    "        timestamp: the timestamp of the execution of the application for naming purposes.\n",
    "    '''\n",
    "    # Create header to bypass Mod_Security\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "    }\n",
    "\n",
    "    # Wait for a few milliseconds so not to be blocked\n",
    "    time.sleep(random.randint(0, 10)/10)\n",
    "\n",
    "    # Get list of news\n",
    "    response = requests.get(url.format(page), headers=headers)\n",
    "\n",
    "    # Verify that response is what was expected\n",
    "    if (response.status_code!=200):\n",
    "        save(os.path.join(errorFolder,\"{}_{}_{:05d}.html\".format(timestamp, source, page)), response.text)\n",
    "        raise ValueError(\"Unexcepcted response code. News page: {:05d}\\n Response Code: {:0d}\"\\\n",
    "            .format(page, response.status_code))\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common variable declarations\n",
    "outputFolder = os.path.join(\"..\", \"dump\")\n",
    "dataFolder = os.path.join(outputFolder, \"staging\")\n",
    "errorFolder = os.path.join(outputFolder, \"errors\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "source = 'https://timesofmalta.com/articles/tags/national/page:{:d}'\n",
    "reg = \"\\{\\\"@context\\\":\\\"http://schema.org\\\",\\\"@graph\\\":\\[\\{\\\"@type\\\":\\\"NewsArticle\\\"(.*?)\\}\\]\\}\\]\\}\\]\\}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(dataFolder, \"tom_links_{}.txt\".format(timestamp)), \"a\", encoding=\"utf-8\") as f:\n",
    "    links = list()\n",
    "\n",
    "    for page in tqdm.tqdm(range(1,3)):\n",
    "        try:\n",
    "            # Scrape news list, parse links\n",
    "            response = scrapeNewsList(page, 'ToM', source, errorFolder, timestamp)\n",
    "\n",
    "            pattern = re.compile(reg)\n",
    "\n",
    "            matches = re.findall(pattern, response)\n",
    "\n",
    "            # Iteratre through all found links and add to a list. This can have duplicates but it allows retrieval of most recent and latest.\n",
    "            for match in matches:\n",
    "                f.write(\"{}{}{}\".format(\"{\\\"@context\\\":\\\"http://schema.org\\\",\\\"@graph\\\":[{\\\"@type\\\":\\\"NewsArticle\\\"\", match, \"}]}]}]}\\n\"))\n",
    "                      \n",
    "        except:\n",
    "            traceback.print_exception(*sys.exc_info(), file=open(\"{}_{}_{:06d}.txt\".format(os.path.join(errorFolder, 'ToM'), timestamp,page), \"w\", encoding=\"utf-8\"))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-venv-kernel",
   "language": "python",
   "name": "news-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
