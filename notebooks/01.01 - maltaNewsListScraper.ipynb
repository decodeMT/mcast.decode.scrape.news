{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "from git import Repo\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "PATH_OF_GIT_REPO = os.path.join(\"..\", \".git\")  # make sure .git folder is properly configured\n",
    "COMMIT_MESSAGE = 'News scrape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gitPull():\n",
    "    '''\n",
    "    Retrieves latest from repository.\n",
    "    '''\n",
    "    try:\n",
    "        repo = Repo(PATH_OF_GIT_REPO)\n",
    "        repo.git.pull()\n",
    "    except:\n",
    "        print('Some error occured while pulling the code') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gitPush(timestamp: str):\n",
    "    '''\n",
    "    Commits to repository\n",
    "    '''\n",
    "    try:\n",
    "        repo = Repo(PATH_OF_GIT_REPO)\n",
    "        repo.git.pull()\n",
    "        repo.git.add(update=True)\n",
    "        repo.index.commit(\"{} {}\".format(COMMIT_MESSAGE, timestamp))\n",
    "        origin = repo.remote(name='origin')\n",
    "        origin.push()\n",
    "    except:\n",
    "        print('Some error occured while pushing the code') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(fileName: str, response: str):\n",
    "    '''\n",
    "    Saves an HTTP request response to file.\n",
    "    Arguments:\n",
    "        fileName: the name with path where to save.\n",
    "        response: the HTTP request response to save.\n",
    "    '''\n",
    "    # Open file and write file\n",
    "    with open(fileName, \"w\", encoding=\"utf-8\") as pf:\n",
    "        pf.write(\"{}\\n\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeNewsList(page: int, source: str, url: str, errorFolder: str, timestamp: str) -> str:\n",
    "    '''\n",
    "    A basic scraper for news links.\n",
    "    Arguments:\n",
    "        page: the page number being scraped.\n",
    "        source: the name of the source.\n",
    "        url: the URL to be scraped. Source specific, with page to be injected.\n",
    "        errorFolder: the folder where errors are to be saved.\n",
    "        timestamp: the timestamp of the execution of the application for naming purposes.\n",
    "    '''\n",
    "    # Create header to bypass Mod_Security\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "    }\n",
    "\n",
    "    # Wait for a few milliseconds so not to be blocked\n",
    "    time.sleep(random.randint(0, 10)/10)\n",
    "\n",
    "    # Get list of news\n",
    "    response = requests.get(url.format(page), headers=headers)\n",
    "\n",
    "    # Verify that response is what was expected\n",
    "    if (response.status_code!=200):\n",
    "        save(os.path.join(errorFolder,\"{}_{}_{:05d}.html\".format(timestamp, source, page)), response.text)\n",
    "        raise ValueError(\"Unexcepcted response code. News page: {:05d}\\n Response Code: {:0d}\"\\\n",
    "            .format(page, response.status_code))\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLinks(response: str, source: str, regex: str, prefix: str, priorFirst: str | None) -> list:\n",
    "    '''\n",
    "    Parses a news portal response to extract all links.\n",
    "    Arguments:\n",
    "        response: the HTML response with the links.\n",
    "        source: the source identifier\n",
    "        regex: the source specific regular expression to extract the links.\n",
    "        prefix: the prefix to add to the extracted links.\n",
    "        priorFirst: the first link extracted in a prior run.\n",
    "    '''\n",
    "    links = list()\n",
    "    pattern = re.compile(regex)\n",
    "\n",
    "    x = re.findall(pattern, response)\n",
    "\n",
    "    # Iteratre through all found links and add to a list. This can have duplicates but it allows retrieval of most recent and latest.\n",
    "    for link in x:\n",
    "        if source=='ToM':\n",
    "            link = \"{}{}{}\".format(\"{\\\"@context\\\":\\\"http://schema.org\\\",\\\"@graph\\\":[{\\\"@type\\\":\\\"NewsArticle\\\"\", link, \"}]}]}]}\")\n",
    "            jsonObject = ast.literal_eval(link)\n",
    "            articles = jsonObject['@graph']\n",
    "            \n",
    "            for article in articles:\n",
    "                fields = dict(article)\n",
    "                \n",
    "                if not fields.__contains__('url'):\n",
    "                    continue\n",
    "                \n",
    "                fullLink = str(fields.get('url'))\n",
    "                if priorFirst is not None and fullLink==priorFirst:\n",
    "                    break\n",
    "                links.append(fullLink.strip())\n",
    "            \n",
    "            continue\n",
    "\n",
    "        fullLink=\"{}{}\".format(prefix,link).strip()\n",
    "\n",
    "        if priorFirst is not None and fullLink==priorFirst:\n",
    "            break\n",
    "\n",
    "        # Some source specific cleaning\n",
    "        if source == 'IN':\n",
    "            start_pos = fullLink.rfind('/local-news/')\n",
    "            end_pos = fullLink.rfind('-')\n",
    "            if start_pos!=-1 and end_pos!=-1:\n",
    "                fullLink = \"{}/local-news/{}\".format(fullLink[0:start_pos], fullLink[end_pos+1:])\n",
    "        elif source == 'MT':\n",
    "            fullLink = fullLink.replace(' ', '-')\n",
    "            fullLink = fullLink.replace(',', '-')\n",
    "            fullLink = fullLink.replace(';', '-')\n",
    "        elif source == 'TVM' and fullLink.__contains__(' rel=bookmark'):\n",
    "            fullLink = fullLink.replace(' rel=bookmark', '')\n",
    "        \n",
    "        links.append(fullLink.strip())\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common variable declarations\n",
    "outputFolder = os.path.join(\"..\", \"dump\")\n",
    "dataFolder = os.path.join(outputFolder, \"staging\")\n",
    "errorFolder = os.path.join(outputFolder, \"errors\")\n",
    "logFolder = os.path.join(outputFolder, \"logs\")\n",
    "runFolder = os.path.join(outputFolder, \"runs\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "data = pd.DataFrame({\"source\":[], \"pages\":[] , \"articles\": [], \"first\": [], \"last\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'ToM': 'https://timesofmalta.com/articles/tags/national/page:{:d}', \n",
    "           'MT': 'https://www.maltatoday.com.mt/news/national/{:d}/', \n",
    "           'IN': 'https://www.independent.com.mt/local?pg={:d}',\n",
    "           'TVM': 'https://tvmnews.mt/en/ahbarijiet_category/local/page/{:d}/',\n",
    "           'NB': 'https://newsbook.com.mt/en/category/news/local/page/{:d}/'}\n",
    "reg = {\"IN\": \"\\<a href\\=\\\"articles\\/(.*?)\\\"\\>\", \n",
    "       \"MT\": \"a href\\=\\\"\\/news\\/national\\/(.*?)\\\"\\>\\<span\\>\", \n",
    "       \"ToM\": \"\\{\\\"@context\\\":\\\"http://schema.org\\\",\\\"@graph\\\":\\[\\{\\\"@type\\\":\\\"NewsArticle\\\"(.*?)\\}\\]\\}\\]\\}\\]\\}\",\n",
    "       \"TVM\": \"href\\=https\\:\\/\\/tvmnews\\.mt\\/en\\/news\\/(.*?) title=\",\n",
    "       \"NB\": \"class\\=td-module-image\\>\\<div\\nclass\\=td-module-thumb\\>\\<a\\nhref\\=(.*?)rel\\=bookmark\"}\n",
    "prefix = {\"IN\": \"https://www.independent.com.mt/articles/\", \n",
    "          \"MT\": \"https://www.maltatoday.com.mt/news/national/\", \n",
    "          \"ToM\": \"\",\n",
    "          \"TVM\": \"https://tvmnews.mt/en/news/\",\n",
    "          \"NB\": \"\"}\n",
    "\n",
    "history = pd.read_csv(\"links_scraper_history.csv\", encoding=\"utf-8\", sep=\",\")\n",
    "history = history.set_index(\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'TVM': 'https://tvmnews.mt/en/ahbarijiet_category/local/page/{:d}/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logStart(lf):\n",
    "    '''\n",
    "    Log start of application.\n",
    "    Arguments:\n",
    "        lf: the log file.\n",
    "    '''\n",
    "    lf.write(\"[{}] Maltese news links scraper started.\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logSourceStart(lf, source: str):\n",
    "    '''\n",
    "    Log the start of scrapping for a specific source.\n",
    "    Arguments:\n",
    "        lf: the log file.\n",
    "        source: the name of the source.\n",
    "    '''\n",
    "    lf.write(\"[{}] Started scraping source {}.\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logSourceNoLinks(lf, source, page):\n",
    "    '''\n",
    "    Log that no links were returned.\n",
    "    Arguments:\n",
    "        lf: the log file.\n",
    "        source: the name of the source.\n",
    "        page: the page number currently being scraped.\n",
    "    '''\n",
    "    lf.write(\"[{}] Scraping of {} interrupted due to a lack of links found for page {:d}.\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), source, page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLinks(links: set, lf, dataFolder: str, source: str, timestamp: str):\n",
    "    '''\n",
    "    Logs links links that have been scraped.\n",
    "    Arguments:\n",
    "        links: set of links.\n",
    "        lf: the log file.\n",
    "        dataFolder: the data folder to save the links.\n",
    "        source: the name of the source.\n",
    "    '''\n",
    "    # Log and save any links (or lack of)\n",
    "    if links.__len__() > 0:\n",
    "        with open(os.path.join(dataFolder, \"{}_{}.csv\".format(source, timestamp)), \"a\", encoding=\"utf-8\") as pf:\n",
    "            lf.write(\"[{}] Saving links from {} in file: {}\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), source, pf.name))\n",
    "            for link in links:\n",
    "                pf.write(\"{}\\n\".format(link))\n",
    "            lf.write(\"[{}] Saved {:d} links.\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), links.__len__()))\n",
    "    else: lf.write(\"[{}] No links to save for source {}.\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logEnd(runFolder: str, timestamp: str, lf, data: pd.DataFrame, history:pd.DataFrame):\n",
    "    '''\n",
    "    Logs the end of the scraper.\n",
    "    Arguments:\n",
    "        runFolder: The folder where the log of the run is being logged.\n",
    "        timestamp: The timestamp when the application started.\n",
    "        lf: the log file.\n",
    "        data: the data to be saved.\n",
    "        history: a dataframe containing the first link extracted from each source in a prior run.\n",
    "    '''\n",
    "    df = os.path.join(runFolder, \"links_scraper_{}.csv\".format(timestamp))\n",
    "    lf.write(\"[{}] Saving statistics for run in {}.\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), df))\n",
    "    data.to_csv(path_or_buf=df, sep=\",\", index=False, encoding=\"utf-8\", header=True)\n",
    "\n",
    "    lf.write(\"[{}] Updating links scraper history file.\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "    history=history.reset_index()\n",
    "    history.to_csv(\"links_scraper_history.csv\", index=False, encoding=\"utf-8\", header=True)\n",
    "\n",
    "    lf.write(\"[{}] Updating repository.\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "    #gitPush(timestamp)\n",
    "\n",
    "    # End gracefully\n",
    "    lf.write(\"[{}] Maltese news links scraper ended.\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def registerLinks(lf, source:str, page: int, links: set, first: str | None, last: str | None, aLinks: list | None) -> tuple[str, str] | None:\n",
    "    '''\n",
    "    Registers links that have been scraped for a specific source.\n",
    "    Arguments:\n",
    "        lf: the log file.\n",
    "        source: the name of the source.\n",
    "        page: the page number that has been scraped.\n",
    "        links: the set of links scraped so far.\n",
    "        first: the first link that has been scraped.\n",
    "        last: the last link that has been scraped.\n",
    "        aLinks: the links that have been scraped from the specific source page.\n",
    "    '''\n",
    "    if aLinks is None:\n",
    "        logSourceNoLinks(lf, source, page)\n",
    "        return (None, None)\n",
    "\n",
    "    # Second stopping condition: check if links have been returned\n",
    "    if aLinks.__len__() == 0:\n",
    "        logSourceNoLinks(lf, source, page)\n",
    "        return (None, None)\n",
    "    \n",
    "    # Register the first and last scraped link\n",
    "    if first==None:\n",
    "        first = aLinks[0]\n",
    "\n",
    "    last=aLinks[-1]\n",
    "\n",
    "    # Add link in set to remove duplicate\n",
    "    for link in aLinks:\n",
    "        links.add(link)\n",
    "\n",
    "    return (first, last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.13s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(logFolder, \"links_scraper_{}.txt\".format(timestamp)), \"a\", encoding=\"utf-8\") as lf:\n",
    "    logStart(lf)\n",
    "\n",
    "    for source in tqdm.tqdm(sources):\n",
    "        links = set()\n",
    "        first = None\n",
    "        last = None\n",
    "        logSourceStart(lf, source)\n",
    "\n",
    "        # Loop for pages in source until first stopping condition reached, the maximum number of pages.\n",
    "        for page in range(1,int(history.loc[source].maxPages)):\n",
    "            try:\n",
    "                # Scrape news list, parse links\n",
    "                priorFirst = history.loc[source].link\n",
    "                response = scrapeNewsList(page, source, sources[source], errorFolder, timestamp)\n",
    "                aLinks = parseLinks(response, source, reg.get(source), prefix.get(source), priorFirst)                \n",
    "                \n",
    "                # Register extracted links\n",
    "                first, last = registerLinks(lf, source, page, links, first, last, aLinks)\n",
    "\n",
    "                if aLinks is None or first is None or last is None:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                if (priorFirst is not None and aLinks.__contains__(priorFirst)):\n",
    "                    break\n",
    "\n",
    "            except:\n",
    "                traceback.print_exception(*sys.exc_info(), file=open(\"{}_{:06d}.txt\".format(os.path.join(errorFolder, source), page), \"w\", encoding=\"utf-8\"))\n",
    "                break\n",
    "\n",
    "        # Register some statistics for the run.\n",
    "        if first is not None:\n",
    "            history.at[source, 'link'] = first\n",
    "        data = pd.concat([data, pd.DataFrame({\"source\":source, \"pages\":page , \"articles\":links.__len__(), \"first\": first, \"last\": last}, index=[source])])\n",
    "\n",
    "        # Log links\n",
    "        logLinks(links, lf, dataFolder, source, timestamp)\n",
    "\n",
    "    # Save and log all statistics for the run.\n",
    "    logEnd(runFolder, timestamp, lf, data, history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-venv-kernel",
   "language": "python",
   "name": "news-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
