{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a parser of ToM pagination of local news to extract links and published date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(\"..\", \"dump\", \"staging\")\n",
    "\n",
    "# Define your PostgreSQL database connection parameters\n",
    "db_url = \"postgresql://username:password@localhost/decodeMT\"\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseTomLinks(file):\n",
    "    i = 0\n",
    "    df = pd.DataFrame({\"url\":[], \"datepublished\":[]})\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            \n",
    "            jsonObject = ast.literal_eval(line)\n",
    "            articles = jsonObject['@graph']\n",
    "            \n",
    "            for article in articles:\n",
    "                fields = dict(article)\n",
    "                \n",
    "                if not fields.__contains__('url') or not fields.__contains__('datePublished'):\n",
    "                    continue\n",
    "                \n",
    "                url = str(fields.get('url')).strip()\n",
    "                datePublished = fields.get('datePublished')\n",
    "\n",
    "                df = pd.concat([df, pd.DataFrame({\"url\":url, \"datepublished\":datePublished}, index=[i])])\n",
    "                i+=1\n",
    "\n",
    "    # Convert to DB types\n",
    "    df = df.convert_dtypes()\n",
    "\n",
    "    # Dump to DB\n",
    "    df.to_sql('tomlinks', engine, schema='localnews', if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(DATA_FOLDER):\n",
    "    if not file.endswith(\"txt\"):\n",
    "        continue\n",
    "\n",
    "    parseTomLinks(os.path.join(DATA_FOLDER, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\dump\\\\staging\\\\tom_links_20230420_153115.txt'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = os.path.join(DATA_FOLDER, 'tom_links_20230420_153115.txt')\n",
    "parseTomLinks(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-venv-kernel",
   "language": "python",
   "name": "news-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
